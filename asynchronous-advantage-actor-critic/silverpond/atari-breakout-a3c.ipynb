{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI-BREAKOUT WITH A3C IN TENSORFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The focus of this notebook is the deep reinforcement learning algorithm **Asynchronous Actor-Critic Advantage (A3C)** (https://arxiv.org/pdf/1602.01783.pdf). In particular, we'll be focussing on:\n",
    "* A high-level theoretical explanation of A3C in the context of deep reinforcement learning generally; and\n",
    "* An implementation of the algorithm for OpenAI's Atari Breakout environment (https://gym.openai.com/envs/Breakout-v0/) using TensorFlow.\n",
    "\n",
    "I've included a quick Background on reinforcement learning, but this should be treated more like an appendix (it doesn't really contain any code, just runs over some theory). The real substance of this notebook is in the third section (3. Asynchronous Actor-Critic Advantage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Background\n",
    "\n",
    "This section is included for the benefit of any reader that needs a quick primer on or introduction to deep reinforcement learning. If you're already pretty comfortable with the topic, then feel free to skip it (i.e. you've got working definitions of the terms \"agent\", \"environment\", \"reward\", \"policy\", and \"value function\"). Otherwise, it's worth reading for the definitions these keywords and a high-level intuition of deep reinforcement learning algorithms. We'll need those before we move on to A3C.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is an area within machine learning concerned with how *agents* learn to *act* in an *environment* in order to maximize some comulative *reward*. In our Atari Breakout example, you can think of the agent as being the player (or computer); the possible actions as being move-left, move-right, or do-nothing; the environment as being the game itself; and rewards as being the points you accumulate (or some penalty you receive for losing).\n",
    "\n",
    "Translating this interaction into pseudocode, it would look something like this:\n",
    "\n",
    "    player starts game\n",
    "    initialize points at 0\n",
    "    while the game is still running:\n",
    "        player looks at the screen\n",
    "        player chooses action (move-left, move-right, do-nothing) by evaluating the current screen\n",
    "        player executes the chosen action\n",
    "        player receives reward (0, 1 point, or loses a life)\n",
    "        increment points by reward\n",
    "\n",
    "And translating this to agent-environment interactions generally:\n",
    "    \n",
    "    initialize agent and environment\n",
    "    initialize accumulated_reward at 0\n",
    "    for each timestep:\n",
    "        agent observes current state of the environment\n",
    "        agent chooses an action based on current state\n",
    "        agent executes action, causing environment to transition to next state\n",
    "        agent receives reward (can be zero or negative) for reaching next state\n",
    "        increment accumulated_reward by reward\n",
    "        \n",
    "One major complication associated with reinforcement learning is delayed rewards. Taking a particular action in a given state may not generate reward immediately, the reward may be lagged by several transitions. Because of this, reinforcement learning relies on the notion of \"discounted reward\": think of the value associated with a given state as being the sum of all rewards that will be accumulated between that state and the terminal state, where each reward in the sequence discounted by some factor (conventionally called \"gamma\").\n",
    "\n",
    "### Problem: Choosing an Action\n",
    "\n",
    "You might already have the question on your mind: **how does agent the choose which action to execute in the current environment state**. This is the **core problem in reinforcement learning**: what we're trying to do is come up with some algorithm that will - over time - let agents figure out what actions they should take in each possible state.\n",
    "\n",
    "If we could somehow come up with a **function that takes the current environment state as input, and outputs a numerical valuation of each of the possible actions** (let's call these \"valuation functions\"), then the solution would be trivial. The agent could use the valuation function and current state to estimate the value of each possible action, then simply **choose whichever action has the highest valuation**.\n",
    "\n",
    "The bad news is that it's really difficuly to actually come up with these valuation functions. So how do we get around that?\n",
    "\n",
    "### Solution: Neural Networks\n",
    "\n",
    "This is where the \"deep\" part of reinforcement learning comes in: **without even knowing the definition of a given valuation function, we can approximate it using a neural network**. This is the approach that we take with A3C. Our problem is now reduced to defining:\n",
    "\n",
    "1. The Network Architecture (a neural network which will function as a good approximator); and\n",
    "2. The Training Regime (the loss function and optimizer, and the method for passing training samples through the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Asynchronous Actor-Critic Advantage (A3C)\n",
    "\n",
    "### Conceptual Overview\n",
    "\n",
    "Before we dive into the code, let's start off by briefly elaborating how A3C fits into the deep reinforcement learning paradigm, and identify what makes it unique from other reinforcement learning algorithms. A good way to do this is to talk about each of the four terms in its name.\n",
    "\n",
    "* **Actor**. Recall that agents in reinforcement learning problems choose which action to take in each given state by calling some kind of \"valuation function\", which assigns a numerical value to each of the actions it could take. The A3C network approximates two different kinds of valuation functions. The first is a \"policy\", $ \\pi(s) $. This is a function which takes a state $s$ as input and outputs a probability distribution over the action space. A3C construes the probabilities as valuations, such that the action which is assigned the highest probability is the action expected to generate the highest reward. Agents use this function to determine which action it should take out of the current state. It is hence referred to as the \"actor\".\n",
    "\n",
    "\n",
    "* **Critic**. The same A3C network is also used approximate a \"value function\", $V(s)$ (yep, that's right, one network is being used to approximate two different functions). A value function takes a state $s$ as input and outputs a numeric estimate of the value of being in that state. This is actually used in the training stage to evaluate the actions suggested by the policy (the \"Actor\"). The \"actor\" (i.e. policy) recommends an action which causes the environment to transition to a new state, then the value function estimates how valuable it is to actually be in that new state (that is, it \"critiques\" the choice made by the policy).\n",
    "\n",
    "\n",
    "* **Advantage**. This is also used in the training stage for the network. Defining this precisely requires a more in-depth discussion about value functions, policies, and Q-values (which we don't touch on here), as well as the relationship between the three. Here, suffice to say that \"advantage\" is (loosely) defined as the difference between the actual and expected values of being in a particular state. We can estimate actual value by computing the sum of the discounted stream of rewards between any given state and the terminal state (more on this later); the expected value is generated by the \"critic\".\n",
    "\n",
    "\n",
    "* **Asynchronous**. A3C is asynchronous in that it generates training samples by running multiple agents/environments on different threads simultaneously. These samples are used to train the single global neural network approximating the policy and value functions. This has the advantage of diversifying the training set, since the experiences of each of those agents will be independent of each other (this was a problem in predecessors of A3C).\n",
    "\n",
    "We should also clarify exactly what the interaction between the global network and any one of the independent agents. Say, for example, that an agent is in an environment at state $s_0$. The agent takes an observation of $s_0$, and uses that observation in a call to the global network (policy approximator) to get an estimate of the action valuations in $s_0$. It then chooses the action $a$ with the highest valuation, executes the action, and transitions to the next state $s_1$. The agent then receives a reward $r$ for reaching $s_1$. The four-tuple $(s_1, a, r, s_0)$ (called a \"transition\") constitutes a single training datum. Once an agent has collected enough transitions, it will send them in a training batch over to the global network, which can be used to update the network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Overview\n",
    "\n",
    "At this point, we have enough to write out the scaffold for the major components of our implementation. The system should be comprised of:\n",
    "\n",
    "* **A single master node**, which (1) hosts the neural network (2) receives and caches training samples from workers, (3) trains the neural network at intervals, and (4) supports policy queries from the workers; and\n",
    "\n",
    "        class Master:\n",
    "            initialize_network()\n",
    "            cache_training_sample()\n",
    "            train_network()\n",
    "            predict_policy()\n",
    "            \n",
    "\n",
    "* **Several drone nodes**, each of which (1) collects training samples by running an agent in an independent copy of the environment, (2) chooses which actions to take in each possible environment state, and (3) sends the training samples it has collected over to the master node at regular intervals. The drones should be implemented as Threads to support arbitrarily many asynchronous duplicates.\n",
    "\n",
    "        class Drone: Thread\n",
    "            run_episode()\n",
    "            choose_action()\n",
    "            send_sample_to_master()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**. Let's start off by importing the packages that we're going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # to define the neural network\n",
    "import numpy as np # manipulate tensors and arrays of training samples\n",
    "from threading import Thread, Lock # to make each of the drones asynchronous\n",
    "import gym # the OpenAI API to run the Atari Breakout emulator\n",
    "import cv2 # image manipulation functions for preprocessing\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Constants**. The next thing we'll do is define a bunch of global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/load location for our neural networks\n",
    "SAVE_DIR = \"saved-networks\"\n",
    "SAVE_FILE = \"network\"\n",
    "\n",
    "# the save frequency (in tf global_steps)\n",
    "SAVE_FREQ = 100\n",
    "\n",
    "# number of independent drones\n",
    "NUM_DRONES = 4\n",
    "THREAD_DELAY = 0.001\n",
    "\n",
    "# environmental details\n",
    "ENVIRONMENT = \"Breakout-v0\" # used to select the Atari Breakout emulator\n",
    "tmp = gym.make(ENVIRONMENT) # create tmp environment to collect metadata\n",
    "RAW_STATE_SHAPE = tmp.observation_space.shape # dimensions of the raw Atari Breakout (state) screen\n",
    "NUM_ACTIONS = tmp.action_space.n # number actions that can be taken in the game\n",
    "STATE_SHAPE = (84, 84, 1) # the dimensions of the preprocessed states\n",
    "NONE_STATE = np.zeros(STATE_SHAPE) # a dummy state, used as a filler for terminal states\n",
    "\n",
    "# discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# number of lags over which discounted reward will be computed\n",
    "N_STEP = 8\n",
    "GAMMA_N = GAMMA ** N_STEP\n",
    "\n",
    "# penalty (negative reward) applied when the agent loses a game \n",
    "FAILURE_PENALTY = -5.0\n",
    "\n",
    "# the reward received by the agent for simply keeping the game going (i.e. not losing)\n",
    "LIVENESS_REWARD = 0.01\n",
    "\n",
    "# epsilon-greedy training parameters\n",
    "# to support trial-end-error, the agent will - at each timestep - choose a random action with probability epsilon\n",
    "EPSILON_INIT = 0.50\n",
    "EPSILON_STOP = 0.15\n",
    "\n",
    "# decrement in epsilon at each training step (slowly decreases the likelihood of choosing a random action)\n",
    "EPSILON_STEP = 0.000005\n",
    "\n",
    "# minimum size of a training batch\n",
    "MIN_TRAINING_BATCH = 32\n",
    "\n",
    "# the update rate for network weights during back-propagation\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "# weights over different components of the loss function used in training (more on this later)\n",
    "COEF_LOSS_V = 0.5\n",
    "COEF_LOSS_ENT = 0.01\n",
    "\n",
    "# flags to identify whether a given transition is into a terminal state\n",
    "MASK_NON_TERMINAL = 1.0\n",
    "MASK_TERMINAL = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Helpers**. Define a few helper functions that we'll use to help write certain tensorflow variables and operations when it comes to defining the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Creates a tensorflow variable.\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Creates a tensorflow bias variable.\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape))\n",
    "\n",
    "def conv_2d(x, W, stride):\n",
    "    \"\"\"\n",
    "    Creates a convolution over the given input tensor (x), using the given weight variable (W).\n",
    "    \"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1,stride,stride,1], padding=\"VALID\")\n",
    "\n",
    "def max_pool(x, stride):\n",
    "    \"\"\"\n",
    "    Max pooling over the given input tensor (x).\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1,stride,stride,1], strides=[1,stride,stride,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**. So far we haven't really talked about preprocessing. This is what we'll do here. The raw Atari Breakout screen is a 210 (height) x 100 (width) x 3 (RGB) pixel vector. This is pretty computationally strenuous to pass through a neural network; on top of that, it contains a lot of redundant information (like the different colours of pixels, and the border). We preprocess the raw images by downsampling them to 84 (height) x 84 (width) and converting to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    \"\"\"\n",
    "    Preprocessing.\n",
    "    \"\"\"\n",
    "    # convert to grayscale\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # then convert to binary\n",
    "    _, state = cv2.threshold(state, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # resize to 84 x 110\n",
    "    state = cv2.resize(state, (84,110))\n",
    "    \n",
    "    # then crop to 84 x 84\n",
    "    state = state[26:110, :]\n",
    "    state = np.reshape(state, (84,84,1))\n",
    "    \n",
    "    #cv2.imshow(\"img\",state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Master**. Fill out the scaffold for the Master node that we've defined above. Pay close attention to the the `initialize_network()` function, which defines both the network architecture of the policy/value approximator as well as the loss and optimization operations for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Master(object):\n",
    "    \"\"\"\n",
    "    Implementation of the A3C Master node. Responsible for:\n",
    "        (1) Hosting the neural network\n",
    "        (2) Receiving and caching training samples from workers\n",
    "        (3) Training the neural network\n",
    "        (4) Support policy queries from workers\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # training cache, stores (s, a, r, s_, mask) transitions received from workers\n",
    "        self.cache = [ [],[],[],[],[] ]\n",
    "        \n",
    "        # used to lock access to the training cache\n",
    "        self.lock = Lock()\n",
    "\n",
    "        # initialize tf network\n",
    "        self.sess = tf.Session()\n",
    "        self.initialize_architecture()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # load an existing network, if one exists\n",
    "        self.load_network()\n",
    "\n",
    "        \n",
    "    def initialize_architecture(self):\n",
    "        \"\"\"\n",
    "        Initializes the neural network that approximates the policy and value functions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # (1) NETWORK ARCHITECTURE\n",
    "        # Adapted from Minh et al. 2015 (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "        #    * State Input : 84 x 84 x 1 pixel matrix\n",
    "        #    * Convolutional Layer 1 : 8x8 convolution, 16 output channels, 4 stride, ReLU activation, 2x2 max pooling\n",
    "        #    * Convolutional Layer 2 : 4x4 convolution, 32 output channels, 2 stride, ReLU activation\n",
    "        #    * Fully Connected Layer : 512 input units, 256 output units, ReLU activation\n",
    "        #    * LSTM Layer : 256 input cells, 256 output cells\n",
    "        #    * Policy Output : 256 input cells (from LSTM), NUM_ACTIONS output cells\n",
    "        #    * Value Output : 256 input cells (from LSTM), 1 output cell\n",
    "        \n",
    "        # input placeholders\n",
    "        with tf.name_scope(\"input\"):\n",
    "            # state tensor, input to the policy and value functions\n",
    "            self.s = tf.placeholder(tf.float32, shape=[None,84,84,1], name=\"state\")\n",
    "            \n",
    "            # action and reward tensor, used during training\n",
    "            self.a = tf.placeholder(tf.float32, shape=[None,NUM_ACTIONS], name=\"action\")\n",
    "            self.r = tf.placeholder(tf.float32, shape=[None,1], name=\"reward\")\n",
    "\n",
    "        # first convolutional layer\n",
    "        with tf.name_scope(\"convolutional-1\"):\n",
    "            # 8x8 convolution, 16 output channels, 4 strides\n",
    "            self.conv1_W = weight_variable([8,8,1,16])\n",
    "            self.conv1_b = bias_variable([16])\n",
    "            self.conv1 = conv_2d(self.s, self.conv1_W, 4) + self.conv1_b\n",
    "            \n",
    "            # passed through ReLU layer\n",
    "            self.conv1 = tf.nn.relu(self.conv1)\n",
    "            \n",
    "            # passed through 2x2 max pooling\n",
    "            self.conv1 = max_pool(self.conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        with tf.name_scope(\"convolutional-2\"):\n",
    "            # 4x4 convolution, 32 output channels, 2 strides\n",
    "            self.conv2_W = weight_variable([4,4,16,32])\n",
    "            self.conv2_b = weight_variable([32])\n",
    "            self.conv2 = conv_2d(self.conv1, self.conv2_W, 2) + self.conv2_b\n",
    "            \n",
    "            # passed through ReLU layer\n",
    "            self.conv2 = tf.nn.relu(self.conv2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        with tf.name_scope(\"fully-connected\"):\n",
    "            # flatten the 16x32 output from the second convolutional layer\n",
    "            self.conv2_flat = tf.reshape(self.conv2, [-1,16*32])\n",
    "            \n",
    "            # 512 input cells, 256 outputs\n",
    "            self.fc_W = weight_variable([16*32,256])\n",
    "            self.fc_b = bias_variable([256])\n",
    "            self.fc = tf.matmul(self.conv2_flat, self.fc_W) + self.fc_b\n",
    "            \n",
    "            # passed through ReLU layer\n",
    "            self.fc = tf.nn.relu(self.fc)\n",
    "\n",
    "        # lstm cell\n",
    "        with tf.name_scope(\"lstm\"):\n",
    "            # 256 input cells\n",
    "            self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256)\n",
    "            self.fc_seq = tf.expand_dims(self.fc, axis=1)\n",
    "            init_state = self.lstm_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "            self.lstm, _ = tf.nn.dynamic_rnn(self.lstm_cell, self.fc_seq, initial_state=init_state, time_major=True)\n",
    "            \n",
    "            # flatten lstm output\n",
    "            self.lstm_flat = tf.reshape(self.lstm, [-1,256])\n",
    "\n",
    "        # output layer for policy approximator\n",
    "        with tf.name_scope(\"output-policy\"):\n",
    "            # 256 input cells, NUM_ACTIONS outputs (the probability distribution)\n",
    "            self.policy_W = weight_variable([256,NUM_ACTIONS])\n",
    "            self.policy_b = bias_variable([NUM_ACTIONS])\n",
    "            self.policy = tf.matmul(self.lstm_flat, self.policy_W) + self.policy_b\n",
    "            \n",
    "            # apply softmax to normalize into a probability distribution\n",
    "            self.policy = tf.nn.softmax(self.policy)\n",
    "\n",
    "        # output layer for value function approximator\n",
    "        with tf.name_scope(\"output-value\"):\n",
    "            # 256 input cells, single output (the state value)\n",
    "            self.value_W = weight_variable([256,1])\n",
    "            self.value_b = weight_variable([1])\n",
    "            self.value = tf.matmul(self.lstm_flat, self.value_W) + self.value_b\n",
    "\n",
    "        \n",
    "        # (2) LOSS OPERATION\n",
    "        # Adapted from Minh et al. 2016 (https://arxiv.org/pdf/1602.01783.pdf)\n",
    "        \n",
    "        # policy loss operation\n",
    "        with tf.name_scope(\"policy-loss\"):\n",
    "            self.log_ap = tf.log(tf.reduce_sum(self.policy*self.a, axis=1, keepdims=True) + 1e-10)\n",
    "            self.advantage = self.r - self.value\n",
    "            self.loss_p = - self.log_ap * tf.stop_gradient(self.advantage)\n",
    "\n",
    "        # value loss operation\n",
    "        with tf.name_scope(\"value-loss\"):\n",
    "            self.loss_v = COEF_LOSS_V * tf.square(self.advantage)\n",
    "\n",
    "        # entropy loss operation\n",
    "        # Minh et al. 2016 found that including this loss function minimized the likelihood\n",
    "        #   of convergence of the network to a suboptimal policy\n",
    "        with tf.name_scope(\"entropy-loss\"):\n",
    "            self.loss_e = COEF_LOSS_ENT * tf.reduce_sum(self.policy * tf.log(self.policy + 1e-10),\n",
    "                                                        axis=1, keepdims=True)\n",
    "\n",
    "        # combined loss operation\n",
    "        with tf.name_scope(\"total-loss\"):\n",
    "            self.loss_t = tf.reduce_mean(self.loss_p + self.loss_v + self.loss_e)\n",
    "\n",
    "        # global step variable, keeps track of number of training runs\n",
    "        with tf.name_scope(\"global-step\"):\n",
    "            self.global_step = tf.Variable(0, name=\"step\", trainable=False)\n",
    "\n",
    "            \n",
    "        # (3) OPTIMIZER OPERATION\n",
    "        # Adapted from Minh et al. 2016 (https://arxiv.org/pdf/1602.01783.pdf))\n",
    "        \n",
    "        # optimizer operation\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, decay=0.99)\n",
    "            self.minimize = self.optimizer.minimize(self.loss_t, global_step=self.global_step)\n",
    "\n",
    "            \n",
    "    def cache_training_sample(self, s, a, r, s_):\n",
    "        \"\"\"\n",
    "        Used by drones to add training samples to the master cache.\n",
    "        \"\"\"\n",
    "        # make sure that no other thread can touch the cache - necessary to prevent race conditions\n",
    "        with self.lock:\n",
    "            # add s, a, and r to their respective queues in the cache\n",
    "            self.cache[0].append(preprocess(s))\n",
    "            self.cache[1].append(a)\n",
    "            self.cache[2].append(r)\n",
    "            \n",
    "            # if s_ is a terminal state ...\n",
    "            if s_ is None:\n",
    "                # ... add the dummy state to the cache\n",
    "                self.cache[3].append(NONE_STATE)\n",
    "                self.cache[4].append(MASK_TERMINAL)\n",
    "            else:\n",
    "                # otherwise, add s_ to the cache\n",
    "                self.cache[3].append(preprocess(s_))\n",
    "                self.cache[4].append(MASK_NON_TERMINAL)\n",
    "\n",
    "                \n",
    "    def train_network(self):\n",
    "        \"\"\"\n",
    "        Training pass over the neural network.\n",
    "            (1) Pulls all training samples from the master cache\n",
    "            (2) Trains the network on these samples\n",
    "        \"\"\"\n",
    "        # check that the cache has sufficient data for a training pass\n",
    "        if len(self.cache[0]) < MIN_TRAINING_BATCH:\n",
    "            return\n",
    "\n",
    "        # pull all data from the cachce\n",
    "        with self.lock:\n",
    "            s, a, r, s_, mask = self.cache\n",
    "            self.cache = [ [],[],[],[],[] ]\n",
    "        s = np.array(s)\n",
    "        a = np.vstack(a)\n",
    "        r = np.vstack(r)\n",
    "        s_ = np.array(s_)\n",
    "        mask = np.vstack(mask)\n",
    "\n",
    "        # generate the value predictions for every state s_\n",
    "        v = self.sess.run(self.value, feed_dict={self.s : s_})\n",
    "        \n",
    "        # add discounted reward for reaching s_ to the reward training vector\n",
    "        #   notice how we use mask here, mask will be 0.0 for terminal states, 1.0 otherwise\n",
    "        #   this ensures that no reward is added for reaching terminal states\n",
    "        r = r + GAMMA_N * v * mask\n",
    "\n",
    "        # call the train operation\n",
    "        self.sess.run(self.minimize, feed_dict={self.s : s, self.a : a, self.r : r})\n",
    "\n",
    "        # save the network\n",
    "        if self.sess.run(self.global_step) % SAVE_FREQ == 0:\n",
    "            self.save_network()\n",
    "            \n",
    "            \n",
    "    def predict_policy(self, s):\n",
    "        \"\"\"\n",
    "        Used by drones to fetch the probability distribution over actions for a given state.\n",
    "        \"\"\"\n",
    "        # predict the policy distribution of a given state\n",
    "        policy = self.sess.run([self.policy], feed_dict={self.s : [s]})\n",
    "        policy = np.squeeze(policy, axis=1)\n",
    "        return policy\n",
    "\n",
    "    \n",
    "    def load_network(self):\n",
    "        \"\"\"\n",
    "        Load network weights from external file.\n",
    "        \"\"\"\n",
    "        checkpoint = tf.train.latest_checkpoint(\n",
    "            checkpoint_dir=SAVE_DIR)\n",
    "        if not (checkpoint is None):\n",
    "            print(\"Existing network found at \" + SAVE_DIR + \". Loading ...\")\n",
    "            self.saver.restore(self.sess, checkpoint)\n",
    "            print(\"... loaded.\")\n",
    "        else:\n",
    "            print(\"No network found. New network initialized.\")\n",
    "\n",
    "            \n",
    "    def save_network(self):\n",
    "        \"\"\"\n",
    "        Save network weights to external file.\n",
    "        \"\"\"\n",
    "        print(\"Saving network ...\")\n",
    "        self.saver.save(self.sess, SAVE_DIR + \"/\" + SAVE_FILE,\n",
    "            global_step=self.global_step)\n",
    "        print(\"... saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drone**. And now we'll implement the scaffold for the Drone class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drone(Thread):\n",
    "    \"\"\"\n",
    "    Implementation of a Drone. Each Drone encapsulates an independent environment/agent.\n",
    "        (1) Collects training samples by running the agent in the environment;\n",
    "        (2) Chooses actions in each state by querying the Master node;\n",
    "        (3) Sends training samples to the Master node at regular intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, master, drone_id, exemplar=False):\n",
    "        super(Drone, self).__init__()\n",
    "        \n",
    "        # reference to the master node\n",
    "        self.master = master\n",
    "        self.sess = self.master.sess\n",
    "        self.drone_id=drone_id\n",
    "        \n",
    "        # this is to mark whether we should render the environment for this Drones\n",
    "        #   only one of the Drones can be an exemplar (i.e. render the Atari emulator at once)\n",
    "        self.exemplar = exemplar\n",
    "        \n",
    "        # launch the Atari environment\n",
    "        self.env = gym.make(ENVIRONMENT)\n",
    "        \n",
    "        # likelihood of choosing a random action and ignoring the policy\n",
    "        self.epsilon = EPSILON_INIT\n",
    "        \n",
    "        # local training cache, regularly passed to master\n",
    "        self.training_samples = []\n",
    "        \n",
    "        # used to accumulate discounted reward over a sequence of transitions\n",
    "        self.R = 0.0\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Implementation of run() method required by Thread. Repeatedly runs training episodes.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            self.run_episode()\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"\n",
    "        Runs a single training episode.\n",
    "        \"\"\"\n",
    "        # reset the emulator, and take an observation of the start screen\n",
    "        s = self.env.reset()\n",
    "        \n",
    "        # accumulator for discounted reward over the episode\n",
    "        episode_R = 0\n",
    "        while True:\n",
    "            time.sleep(THREAD_DELAY) # yield control to other threads\n",
    "            \n",
    "            # render the emulator if this drone is the exemplar\n",
    "            # NOTE: this doesn't work in Notebook\n",
    "            #if self.exemplar:\n",
    "            #    self.env.render()\n",
    "\n",
    "            # choose an action\n",
    "            a = self.act(s)\n",
    "            \n",
    "            # execute the action, and collect the transition variables\n",
    "            s_, r, end, _ = self.env.step(a)\n",
    "            if end:\n",
    "                s_ = None\n",
    "                r = FAILURE_PENALTY\n",
    "            elif r < 0.001:\n",
    "                r = LIVENESS_REWARD\n",
    "            \n",
    "            # convert the action to one-hot representation (necessary for tensorflow)\n",
    "            a_onehot = np.zeros(NUM_ACTIONS)\n",
    "            a_onehot[a] = 1\n",
    "\n",
    "            # save the transition in local memory\n",
    "            transition = (s, a_onehot, r, s_)\n",
    "            self.training_samples.append(transition)\n",
    "\n",
    "            # update the accumulated discounted reward\n",
    "            self.R = (self.R + r * GAMMA_N) / GAMMA\n",
    "\n",
    "            # if we've hit the terminal state\n",
    "            if s_ is None:\n",
    "                while len(self.training_samples) > 0:\n",
    "                    n = len(self.training_samples)\n",
    "                    s, a, r, s_ = self.sample_memory(n)\n",
    "                    self.master.cache_training_sample(s, a, r, s_)\n",
    "                    self.R = (self.R - self.training_samples[0][2]) / GAMMA\n",
    "                    self.training_samples.pop(0)\n",
    "                self.R = 0\n",
    "\n",
    "            # if the local cache has reached capacity\n",
    "            if len(self.training_samples) >= N_STEP:\n",
    "                s, a, r, s_ = self.sample_memory(N_STEP)\n",
    "                self.master.cache_training_sample(s, a, r, s_)\n",
    "                self.R = self.R - self.training_samples[0][2]\n",
    "                self.training_samples.pop(0)\n",
    "\n",
    "            # transition to the next state\n",
    "            s = s_\n",
    "            \n",
    "            # accumulate total episode reward\n",
    "            episode_R += r\n",
    "\n",
    "            # end the episode once we've reached a terminal state\n",
    "            if end:\n",
    "                break\n",
    "\n",
    "        # record episode reward for the exemplar\n",
    "        if self.drone_id == 0:\n",
    "            log = open(\"log-a3c-\"+str(self.drone_id)+\".txt\", \"a\")\n",
    "            log.write(str(episode_R) + \"\\n\")\n",
    "            print(\"Episode Reward: \" + str(episode_R))\n",
    "     \n",
    "    \n",
    "    def sample_memory(self, n):\n",
    "        \"\"\"\n",
    "        Fetches an n-step transition from local memory.\n",
    "        \"\"\"\n",
    "        s, a, _, _ = self.training_samples[0]\n",
    "        _, _, _, s_ = self.training_samples[n-1]\n",
    "        return s, a, self.R, s_\n",
    "    \n",
    "    \n",
    "    def act(self, s):\n",
    "        \"\"\"\n",
    "        Chooses an action in a given state, by querying the Master for a policy.\n",
    "        \"\"\"\n",
    "        # with probability epsilon, choose a random action\n",
    "        if random.random() < self.epsilon:\n",
    "            # ... and if we do that, make sure we decrement epsilon\n",
    "            if self.epsilon > EPSILON_INIT:\n",
    "                self.epsilon -= EPSILON_STEP\n",
    "            return random.randint(0, NUM_ACTIONS-1)\n",
    "        \n",
    "        # otherwise, ask the master for a policy\n",
    "        else:\n",
    "            policy = self.master.predict_policy(preprocess(s))\n",
    "            # choose an action in accordance with the policy\n",
    "            # Note, an alternative would be to choose the policy with the highest probability\n",
    "            return np.random.choice(NUM_ACTIONS, p=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(Thread):\n",
    "    \"\"\"\n",
    "    Simple optimizer class. All this does is repeatedly call train_network() in master.\n",
    "    \"\"\"\n",
    "    def __init__(self, master):\n",
    "        super(Optimizer, self).__init__()\n",
    "        self.master = master\n",
    "    \n",
    "    def run(self):\n",
    "        while True:\n",
    "            self.master.train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # initialize master\n",
    "    master = Master()\n",
    "    \n",
    "    # initialize the drones\n",
    "    drones = [Drone(master, drone_id=i) for i in range(NUM_DRONES-1)]\n",
    "    \n",
    "    # make the first drone an exemplar\n",
    "    drones[0].exemplar = True\n",
    "    \n",
    "    for drone in drones:\n",
    "        drone.start()\n",
    "        \n",
    "    optimizer = Optimizer(master)\n",
    "    optimizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note about running this code in the Notebook:\n",
    "* Jupyter Notebook doesn't support rendering for the Atari emulator, so - unfortunately - I've had to disable the rendering functionality. This means that you won't be able to watch A3C actually playing Atari Breakout unless you export the code into a .py file and run it from the terminal.\n",
    "* I've run into a few bugs that I haven't implemented fixes for. The most annoying one is that if you run `main()` once, then force stop it, you won't be able to start it again unless you restart the Notebook kernel.\n",
    "* Because A3C can take several hours to converge, you won't be able to make out any trend in the discounted reward figures that are reported when running `main()`. Check out https://www.youtube.com/watch?v=V1eYniJ0Rnk (only 1:30 mins) for how the network should perform as it starts to converge. The video is actually for DQN (A3C's predecessor), but the behaviour should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
